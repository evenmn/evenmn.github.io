
@article{goodfellow2014,
	title = {Generative {Adversarial} {Networks}},
	volume = {63},
	url = {https://arxiv.org/abs/1406.2661v1},
	abstract = {We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a generative
model G that captures the data distribution, and a discriminative model D that
estimates the probability that a sample came from the training data rather than
G. The training procedure for G is to maximize the probability of D making a
mistake. This framework corresponds to a minimax two-player game. In the space
of arbitrary functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1/2 everywhere. In the case where G
and D are defined by multilayer perceptrons, the entire system can be trained
with backpropagation. There is no need for any Markov chains or unrolled
approximate inference networks during either training or generation of samples.
Experiments demonstrate the potential of the framework through qualitative and
quantitative evaluation of the generated samples.},
	number = {11},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Computer Science - Machine Learning, Friction GAN, GAN, Machine Learning, ML, Statistics - Machine Learning, thesis},
	pages = {139--144},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/6VY9FM2R/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/P6MX7PLE/1406.html:text/html;Goodfellow et al. - 2014 - Generative Adversarial Networks:/Users/evenmn/Zotero/storage/R7L6SVPF/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(2):/Users/evenmn/Zotero/storage/X8PRMRWI/Goodfellow et al. - 2014 - Generative Adversarial Networks(2).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(3):/Users/evenmn/Zotero/storage/2JGR6ZPR/Goodfellow et al. - 2014 - Generative Adversarial Networks(3).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(4):/Users/evenmn/Zotero/storage/VB22KLC4/Goodfellow et al. - 2014 - Generative Adversarial Networks(4).pdf:application/pdf},
}

@article{vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {2017-Decem},
	url = {https://arxiv.org/abs/1706.03762v5},
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {Publisher: Neural information processing systems foundation},
	keywords = {Friction GAN, Machine Learning, ML, thesis},
	pages = {5999--6009},
	file = {Vaswani et al. - 2017 - Attention Is All You Need:/Users/evenmn/Zotero/storage/2QPTJRX8/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;Vaswani et al. - 2017 - Attention Is All You Need(2):/Users/evenmn/Zotero/storage/QVGYIC7N/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf:application/pdf},
}

@article{rumelhart1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	doi = {10.1038/323533a0},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	year = {1986},
	keywords = {thesis},
	pages = {533},
}

@article{hornik1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	doi = {10.1016/0893-6080(89)90020-8},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, K. and Stinchcombe, M. and White, H.},
	year = {1989},
	keywords = {thesis},
	pages = {359},
}

@article{krizhevsky2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-04-24},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	keywords = {thesis},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/evenmn/Zotero/storage/C235QH8Q/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{rabinowicz1951,
	title = {The {Nature} of the {Static} and {Kinetic} {Coefficients} of {Friction}},
	volume = {22},
	issn = {0021-8979},
	url = {https://aip.scitation.org/doi/abs/10.1063/1.1699869},
	doi = {10.1063/1.1699869},
	number = {11},
	urldate = {2022-12-04},
	journal = {Journal of Applied Physics},
	author = {Rabinowicz, Ernest},
	month = nov,
	year = {1951},
	note = {Publisher: American Institute of Physics},
	keywords = {Crystal Aging, poster, thesis},
	pages = {1373--1379},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/4YUDQRW6/Rabinowicz - 1951 - The Nature of the Static and Kinetic Coefficients .pdf:application/pdf},
}

@article{dokos1946,
	title = {Sliding {Friction} {Under} {Extreme} {Pressures}—1},
	volume = {13},
	issn = {0021-8936},
	url = {https://doi.org/10.1115/1.4009539},
	doi = {10.1115/1.4009539},
	abstract = {The program of this investigation on sliding friction is concerned with the evaluation of the frictional forces which occur under very high contact pressures and under varying conditions of speed, temperature, and lubrication. The work presented herein covers the experiments which have been made on sliding friction at normal temperatures. Further experiments on boundary lubrication at normal temperatures and sliding friction at elevated temperatures will be reported in a second paper.This report indicates the behavior of sliding friction when large normal loads react between surfaces in contact over a wide range of sliding velocities. At velocities less than one inch per second magnified photographs of the ruptured surfaces indicate clearly this phenomenon of stick-slip. From the experimental records obtained it was possible to show that a functional relation exists between the following variables: (1) The sliding velocity, (2) the normal load, and, (3) the frequency of stick-slip. New experimental evidence is presented indicating that the natural frequency of the apparatus involved influences the phenomenon of stick-slip.},
	number = {2},
	urldate = {2023-02-24},
	journal = {Journal of Applied Mechanics},
	author = {Dokos, S. J.},
	month = jun,
	year = {1946},
	keywords = {Crystal Aging, crystal rsf, thesis},
	pages = {A148--A156},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/T284CEN9/Dokos - 2021 - Sliding Friction Under Extreme Pressures—1.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/GLTZUKDE/Sliding-Friction-Under-Extreme-Pressures-1.html:text/html},
}

@article{nordhagen2023a,
	title = {Diffusion-{Driven} {Frictional} {Aging} in {Silicon} {Carbide}},
	volume = {71},
	issn = {1573-2711},
	url = {https://doi.org/10.1007/s11249-023-01762-z},
	doi = {10.1007/s11249-023-01762-z},
	abstract = {Friction is the force resisting relative motion of objects. The force depends on material properties, loading conditions and external factors such as temperature and humidity, but also contact aging has been identified as a primary factor. Several aging mechanisms have been proposed, including increased “contact quantity” due to plastic or elastic creep and enhanced “contact quality” due to formation of strong interfacial bonds. However, comparatively less attention has been given to other mechanisms that enhance the “contact quantity”. In this study, we explore the influence of crystal faceting on the augmentation of “contact quantity” in cubic silicon carbide, driven by the minimization of surface free energy. Our observations reveal that the temporal evolution of the frictional aging effect follows a logarithmic pattern, akin to several other aging mechanisms. However, this particular mechanism is driven by internal capillary forces instead of the normal force typically associated with friction. Due to this fundamental distinction, existing frictional aging models fail to comprehensively explain the observed behavior. In light of these findings, we derive a model for the evolution of contact area caused by diffusion-driven frictional aging, drawing upon principles from statistical mechanics. Upon application of a normal force, the friction force is increased due to plastic creep. This investigation presents an alternative explanation for the logarithmic aging behavior observed and offers the potential to contribute to the development of more accurate friction models.},
	language = {en},
	number = {3},
	urldate = {2023-07-25},
	journal = {Tribology Letters},
	author = {Nordhagen, Even Marius and Sveinsson, Henrik Andersen and Malthe-Sørenssen, Anders},
	month = jul,
	year = {2023},
	keywords = {crystal rsf, Faceting, Frictional aging, Molecular dynamics, Nanotribology, prl\_paper, Surface diffusion, thesis},
	pages = {95},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/94IQG2TT/Nordhagen et al. - 2023 - Diffusion-Driven Frictional Aging in Silicon Carbi.pdf:application/pdf},
}

@misc{ho2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, ddpm, Friction GAN, Statistics - Machine Learning, thesis},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/NBEBJ8E7/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/ZM6FEYES/2006.html:text/html},
}

@misc{he2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, thesis},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/T8TL94FC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/S6ADEFEU/1512.html:text/html},
}

@misc{lang2024,
	title = {{AIFS} - {ECMWF}'s data-driven forecasting system},
	url = {http://arxiv.org/abs/2406.01465},
	doi = {10.48550/arXiv.2406.01465},
	abstract = {Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting. Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses. It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data. AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data. We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks. AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Lang, Simon and Alexe, Mihai and Chantry, Matthew and Dramsch, Jesper and Pinault, Florian and Raoult, Baudouin and Clare, Mariana C. A. and Lessig, Christian and Maier-Gerber, Michael and Magnusson, Linus and Bouallègue, Zied Ben and Nemesio, Ana Prieto and Dueben, Peter D. and Brown, Andrew and Pappenberger, Florian and Rabier, Florence},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01465 [physics]},
	keywords = {AIFS, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/D5SABF8K/Lang et al. - 2024 - AIFS - ECMWF's data-driven forecasting system.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/HFSTS84Y/2406.html:text/html},
}

@article{bi2023,
	title = {Accurate medium-range global weather forecasting with {3D} neural networks},
	volume = {619},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06185-3},
	doi = {10.1038/s41586-023-06185-3},
	abstract = {Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
	language = {en},
	number = {7970},
	urldate = {2024-05-14},
	journal = {Nature},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {AIFS, Atmospheric dynamics, Computer science},
	pages = {533--538},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/Q6ZH4I69/Bi et al. - 2023 - Accurate medium-range global weather forecasting w.pdf:application/pdf},
}

@article{lillicrap2020,
	title = {Backpropagation and the brain},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2025-04-04},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cortex, Learning algorithms, Long-term potentiation, Network models, Neurophysiology},
	pages = {335--346},
}

@misc{rombach2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/TGWKR5GJ/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/9TUWHGU9/2112.html:text/html},
}

@misc{kingma2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/JVULNHDZ/Kingma og Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/WGUH8ANZ/1312.html:text/html},
}

@inproceedings{kurth2023,
	address = {New York, NY, USA},
	series = {{PASC} '23},
	title = {{FourCastNet}: {Accelerating} {Global} {High}-{Resolution} {Weather} {Forecasting} {Using} {Adaptive} {Fourier} {Neural} {Operators}},
	isbn = {979-8-4007-0190-0},
	shorttitle = {{FourCastNet}},
	url = {https://dl.acm.org/doi/10.1145/3592979.3593412},
	doi = {10.1145/3592979.3593412},
	abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
	urldate = {2025-05-16},
	booktitle = {Proceedings of the {Platform} for {Advanced} {Scientific} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
	month = jun,
	year = {2023},
	pages = {1--11},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/MSHEHGGB/Kurth et al. - 2023 - FourCastNet Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural O.pdf:application/pdf},
}
