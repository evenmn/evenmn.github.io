
@article{goodfellow2014,
	title = {Generative {Adversarial} {Networks}},
	volume = {63},
	url = {https://arxiv.org/abs/1406.2661v1},
	abstract = {We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a generative
model G that captures the data distribution, and a discriminative model D that
estimates the probability that a sample came from the training data rather than
G. The training procedure for G is to maximize the probability of D making a
mistake. This framework corresponds to a minimax two-player game. In the space
of arbitrary functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1/2 everywhere. In the case where G
and D are defined by multilayer perceptrons, the entire system can be trained
with backpropagation. There is no need for any Markov chains or unrolled
approximate inference networks during either training or generation of samples.
Experiments demonstrate the potential of the framework through qualitative and
quantitative evaluation of the generated samples.},
	number = {11},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {Publisher: Association for Computing Machinery},
	keywords = {GAN, ML, Friction GAN, Machine Learning, Computer Science - Machine Learning, Statistics - Machine Learning, thesis},
	pages = {139--144},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/6VY9FM2R/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/P6MX7PLE/1406.html:text/html;Goodfellow et al. - 2014 - Generative Adversarial Networks:/Users/evenmn/Zotero/storage/R7L6SVPF/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(2):/Users/evenmn/Zotero/storage/X8PRMRWI/Goodfellow et al. - 2014 - Generative Adversarial Networks(2).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(3):/Users/evenmn/Zotero/storage/2JGR6ZPR/Goodfellow et al. - 2014 - Generative Adversarial Networks(3).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(4):/Users/evenmn/Zotero/storage/VB22KLC4/Goodfellow et al. - 2014 - Generative Adversarial Networks(4).pdf:application/pdf},
}

@article{vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {2017-Decem},
	url = {https://arxiv.org/abs/1706.03762v5},
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {Publisher: Neural information processing systems foundation},
	keywords = {ML, Friction GAN, Machine Learning, thesis},
	pages = {5999--6009},
	file = {Vaswani et al. - 2017 - Attention Is All You Need:/Users/evenmn/Zotero/storage/2QPTJRX8/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;Vaswani et al. - 2017 - Attention Is All You Need(2):/Users/evenmn/Zotero/storage/QVGYIC7N/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf:application/pdf},
}

@article{rumelhart1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	doi = {10.1038/323533a0},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	year = {1986},
	keywords = {thesis},
	pages = {533},
}

@article{hornik1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	doi = {10.1016/0893-6080(89)90020-8},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, K. and Stinchcombe, M. and White, H.},
	year = {1989},
	keywords = {thesis},
	pages = {359},
}

@article{krizhevsky2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-04-24},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	keywords = {thesis},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/evenmn/Zotero/storage/C235QH8Q/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@misc{ho2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Friction GAN, Computer Science - Machine Learning, Statistics - Machine Learning, thesis, ddpm},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/NBEBJ8E7/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/ZM6FEYES/2006.html:text/html},
}

@misc{he2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, thesis},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/T8TL94FC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/S6ADEFEU/1512.html:text/html},
}

@article{lillicrap2020,
	title = {Backpropagation and the brain},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2025-04-04},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cortex, Learning algorithms, Long-term potentiation, Network models, Neurophysiology},
	pages = {335--346},
}

@misc{rombach2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/TGWKR5GJ/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/9TUWHGU9/2112.html:text/html},
}
