
@article{perlin2002,
	title = {Improving {Noise}},
	doi = {10.1145/566570.566636},
	abstract = {Two deficiencies in the original Noise algorithm are corrected: second order interpolation discontinuity and unoptimal gradient computation. With these defects corrected, Noise both looks better and runs faster. The latter change also makes it easier to define a uniform mathematical reference standard. Copyright © 2002, ACM. All rights reserved.},
	journal = {ACM Transactions on Graphics},
	author = {Perlin, Ken},
	month = jul,
	year = {2002},
	keywords = {Friction GAN, Perlin Noise, Simplex Noise},
	pages = {681--682},
	file = {Perlin - 2002 - Improving Noise:/Users/evenmn/Zotero/storage/GHI99IPI/Perlin - 2002 - Improving Noise.pdf:application/pdf},
}

@article{goodfellow2014,
	title = {Generative {Adversarial} {Networks}},
	volume = {63},
	url = {https://arxiv.org/abs/1406.2661v1},
	abstract = {We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a generative
model G that captures the data distribution, and a discriminative model D that
estimates the probability that a sample came from the training data rather than
G. The training procedure for G is to maximize the probability of D making a
mistake. This framework corresponds to a minimax two-player game. In the space
of arbitrary functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1/2 everywhere. In the case where G
and D are defined by multilayer perceptrons, the entire system can be trained
with backpropagation. There is no need for any Markov chains or unrolled
approximate inference networks during either training or generation of samples.
Experiments demonstrate the potential of the framework through qualitative and
quantitative evaluation of the generated samples.},
	number = {11},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {Publisher: Association for Computing Machinery},
	keywords = {GAN, ML, Friction GAN, Machine Learning, Computer Science - Machine Learning, Statistics - Machine Learning, thesis},
	pages = {139--144},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/6VY9FM2R/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/P6MX7PLE/1406.html:text/html;Goodfellow et al. - 2014 - Generative Adversarial Networks:/Users/evenmn/Zotero/storage/R7L6SVPF/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(2):/Users/evenmn/Zotero/storage/X8PRMRWI/Goodfellow et al. - 2014 - Generative Adversarial Networks(2).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(3):/Users/evenmn/Zotero/storage/2JGR6ZPR/Goodfellow et al. - 2014 - Generative Adversarial Networks(3).pdf:application/pdf;Goodfellow et al. - 2014 - Generative Adversarial Networks(4):/Users/evenmn/Zotero/storage/VB22KLC4/Goodfellow et al. - 2014 - Generative Adversarial Networks(4).pdf:application/pdf},
}

@article{vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {2017-Decem},
	url = {https://arxiv.org/abs/1706.03762v5},
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {Publisher: Neural information processing systems foundation},
	keywords = {ML, Friction GAN, Machine Learning, thesis},
	pages = {5999--6009},
	file = {Vaswani et al. - 2017 - Attention Is All You Need:/Users/evenmn/Zotero/storage/2QPTJRX8/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;Vaswani et al. - 2017 - Attention Is All You Need(2):/Users/evenmn/Zotero/storage/QVGYIC7N/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf:application/pdf},
}

@article{carleo2017,
	title = {Solving the {Quantum} {Many}-{Body} {Problem} with {Artificial} {Neural} {Networks}},
	volume = {355},
	doi = {10.1126/science.aag2302},
	number = {6325},
	journal = {Science (New York, N.Y.)},
	author = {Carleo, G. and Troyer, M.},
	year = {2017},
	pages = {602},
}

@article{rumelhart1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	doi = {10.1038/323533a0},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	year = {1986},
	keywords = {thesis},
	pages = {533},
}

@article{metropolis1949,
	title = {The {Monte} {Carlo} {Method}},
	volume = {44},
	doi = {10.1080/01621459.1949.10483310},
	number = {247},
	journal = {Journal of the American Statistical Association},
	author = {Metropolis, N. and Ulam, S.},
	year = {1949},
	pmid = {18139350},
	pages = {335},
}

@article{hornik1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	doi = {10.1016/0893-6080(89)90020-8},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, K. and Stinchcombe, M. and White, H.},
	year = {1989},
	keywords = {thesis},
	pages = {359},
}

@article{pfau2020,
	title = {Ab initio solution of the many-electron {Schr}{\textbackslash}"odinger equation with deep neural networks},
	volume = {2},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033429},
	doi = {10.1103/PhysRevResearch.2.033429},
	abstract = {Given access to accurate solutions of the many-electron Schrödinger equation, nearly all chemistry could be derived from first principles. Exact wave functions of interesting chemical systems are out of reach because they are NP-hard to compute in general, but approximations can be found using polynomially scaling algorithms. The key challenge for many of these algorithms is the choice of wave function approximation, or Ansatz, which must trade off between efficiency and accuracy. Neural networks have shown impressive power as accurate practical function approximators and promise as a compact wave-function Ansatz for spin systems, but problems in electronic structure require wave functions that obey Fermi-Dirac statistics. Here we introduce a novel deep learning architecture, the Fermionic neural network, as a powerful wave-function Ansatz for many-electron systems. The Fermionic neural network is able to achieve accuracy beyond other variational quantum Monte Carlo Ansatz on a variety of atoms and small molecules. Using no data other than atomic positions and charges, we predict the dissociation curves of the nitrogen molecule and hydrogen chain, two challenging strongly correlated systems, to significantly higher accuracy than the coupled cluster method, widely considered the most accurate scalable method for quantum chemistry at equilibrium geometry. This demonstrates that deep neural networks can improve the accuracy of variational quantum Monte Carlo to the point where it outperforms other ab initio quantum chemistry methods, opening the possibility of accurate direct optimization of wave functions for previously intractable many-electron systems.},
	number = {3},
	urldate = {2022-11-09},
	journal = {Physical Review Research},
	author = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
	month = sep,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {033429},
	file = {APS Snapshot:/Users/evenmn/Zotero/storage/6LV3EALV/PhysRevResearch.2.html:text/html;Full Text PDF:/Users/evenmn/Zotero/storage/3PMAG2EZ/Pfau et al. - 2020 - Ab initio solution of the many-electron Schrodin.pdf:application/pdf},
}

@article{krizhevsky2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-04-24},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	keywords = {thesis},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/evenmn/Zotero/storage/C235QH8Q/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{rabinowicz1951,
	title = {The {Nature} of the {Static} and {Kinetic} {Coefficients} of {Friction}},
	volume = {22},
	issn = {0021-8979},
	url = {https://aip.scitation.org/doi/abs/10.1063/1.1699869},
	doi = {10.1063/1.1699869},
	number = {11},
	urldate = {2022-12-04},
	journal = {Journal of Applied Physics},
	author = {Rabinowicz, Ernest},
	month = nov,
	year = {1951},
	note = {Publisher: American Institute of Physics},
	keywords = {Crystal Aging, thesis, poster},
	pages = {1373--1379},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/4YUDQRW6/Rabinowicz - 1951 - The Nature of the Static and Kinetic Coefficients .pdf:application/pdf},
}

@article{bar-sinai2014,
	title = {On the velocity-strengthening behavior of dry friction},
	volume = {119},
	issn = {2169-9356},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2013JB010586},
	doi = {10.1002/2013JB010586},
	abstract = {The onset of frictional instabilities, e.g., earthquakes nucleation, is intimately related to velocity-weakening friction, in which the frictional resistance of interfaces decreases with increasing slip velocity. While this frictional response has been studied extensively, less attention has been given to steady state velocity-strengthening friction, in spite of its potential importance for various aspects of frictional phenomena such as the propagation speed of interfacial rupture fronts and the amount of stored energy released by them. In this note we suggest that a crossover from steady state velocity-weakening friction at small slip velocities to steady state velocity-strengthening friction at higher velocities might be a generic feature of dry friction. We further argue that while thermally activated rheology naturally gives rise to logarithmic steady state velocity-strengthening friction, a crossover to stronger-than-logarithmic strengthening might take place at higher slip velocities, possibly accompanied by a change in the dominant dissipation mechanism. We sketch a few physical mechanisms that may account for the crossover to stronger-than-logarithmic steady state velocity strengthening and compile a rather extensive set of experimental data available in the literature, lending support to these ideas.},
	language = {en},
	number = {3},
	urldate = {2023-02-25},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Bar-Sinai, Yohai and Spatschek, Robert and Brener, Efim A. and Bouchbinder, Eran},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/2013JB010586},
	keywords = {Crystal Aging, friction, crystal rsf, constitutive laws, thesis, slow rupture, stable slip, velocity-strengthening, velocity shape, prl\_paper},
	pages = {1738--1748},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/TNGJJ5M3/Bar-Sinai et al. - 2014 - On the velocity-strengthening behavior of dry fric.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/XU3VWEWD/2013JB010586.html:text/html},
}

@article{dokos1946,
	title = {Sliding {Friction} {Under} {Extreme} {Pressures}—1},
	volume = {13},
	issn = {0021-8936},
	url = {https://doi.org/10.1115/1.4009539},
	doi = {10.1115/1.4009539},
	abstract = {The program of this investigation on sliding friction is concerned with the evaluation of the frictional forces which occur under very high contact pressures and under varying conditions of speed, temperature, and lubrication. The work presented herein covers the experiments which have been made on sliding friction at normal temperatures. Further experiments on boundary lubrication at normal temperatures and sliding friction at elevated temperatures will be reported in a second paper.This report indicates the behavior of sliding friction when large normal loads react between surfaces in contact over a wide range of sliding velocities. At velocities less than one inch per second magnified photographs of the ruptured surfaces indicate clearly this phenomenon of stick-slip. From the experimental records obtained it was possible to show that a functional relation exists between the following variables: (1) The sliding velocity, (2) the normal load, and, (3) the frequency of stick-slip. New experimental evidence is presented indicating that the natural frequency of the apparatus involved influences the phenomenon of stick-slip.},
	number = {2},
	urldate = {2023-02-24},
	journal = {Journal of Applied Mechanics},
	author = {Dokos, S. J.},
	month = jun,
	year = {1946},
	keywords = {Crystal Aging, crystal rsf, thesis},
	pages = {A148--A156},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/T284CEN9/Dokos - 2021 - Sliding Friction Under Extreme Pressures—1.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/GLTZUKDE/Sliding-Friction-Under-Extreme-Pressures-1.html:text/html},
}

@article{jacobs2013,
	title = {Nanoscale wear as a stress-assisted chemical reaction},
	volume = {8},
	copyright = {2013 Nature Publishing Group},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/nnano.2012.255},
	doi = {10.1038/nnano.2012.255},
	abstract = {Wear of sliding contacts leads to energy dissipation and device failure, resulting in massive economic and environmental costs1. Typically, wear phenomena are described empirically2, because physical and chemical interactions at sliding interfaces are not fully understood at any length scale. Fundamental insights from individual nanoscale contacts are crucial for understanding wear at larger length scales3, and to enable reliable nanoscale devices, manufacturing and microscopy4,5,6. Observable nanoscale wear mechanisms include fracture7 and plastic deformation8, but recent experiments9,10,11 and models12 propose another mechanism: wear via atom-by-atom removal (‘atomic attrition’), which can be modelled using stress-assisted chemical reaction kinetics13. Experimental evidence for this has so far been inferential. Here, we quantitatively measure the wear of silicon—a material relevant to small-scale devices14—using in situ transmission electron microscopy. We resolve worn volumes as small as 25 ± 5 nm3, a factor of 103 lower than is achievable using alternative techniques15,16. Wear of silicon against diamond is consistent with atomic attrition, and inconsistent with fracture or plastic deformation, as shown using direct imaging. The rate of atom removal depends exponentially on stress in the contact, as predicted by chemical rate kinetics13. Measured activation parameters are consistent with an atom-by-atom process17. These results, by direct observation, establish atomic attrition as the primary wear mechanism of silicon in vacuum at low loads.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Nature Nanotechnology},
	author = {Jacobs, Tevis D. B. and Carpick, Robert W.},
	month = feb,
	year = {2013},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Physical chemistry, crystal rsf, thesis, Structural properties, prl\_paper},
	pages = {108--112},
	annote = {New wear mechanism suggested (atomic attrition)
},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/JYLI5KCN/Jacobs and Carpick - 2013 - Nanoscale wear as a stress-assisted chemical react.pdf:application/pdf},
}

@article{kilgore2012,
	title = {Laboratory {Observations} of {Fault} {Strength} in {Response} to {Changes} in {Normal} {Stress}},
	volume = {79},
	issn = {0021-8936},
	url = {https://doi.org/10.1115/1.4005883},
	doi = {10.1115/1.4005883},
	abstract = {Changes in fault normal stress can either inhibit or promote rupture propagation, depending on the fault geometry and on how fault shear strength varies in response to the normal stress change. A better understanding of this dependence will lead to improved earthquake simulation techniques, and ultimately, improved earthquake hazard mitigation efforts. We present the results of new laboratory experiments investigating the effects of step changes in fault normal stress on the fault shear strength during sliding, using bare Westerly granite samples, with roughened sliding surfaces, in a double direct shear apparatus. Previous experimental studies examining the shear strength following a step change in the normal stress produce contradictory results: a set of double direct shear experiments indicates that the shear strength of a fault responds immediately, and then is followed by a prolonged slip-dependent response, while a set of shock loading experiments indicates that there is no immediate component, and the response is purely gradual and slip-dependent. In our new, high-resolution experiments, we observe that the acoustic transmissivity and dilatancy of simulated faults in our tests respond immediately to changes in the normal stress, consistent with the interpretations of previous investigations, and verify an immediate increase in the area of contact between the roughened sliding surfaces as normal stress increases. However, the shear strength of the fault does not immediately increase, indicating that the new area of contact between the rough fault surfaces does not appear preloaded with any shear resistance or strength. Additional slip is required for the fault to achieve a new shear strength appropriate for its new loading conditions, consistent with previous observations made during shock loading.},
	number = {3},
	urldate = {2023-03-14},
	journal = {Journal of Applied Mechanics},
	author = {Kilgore, Brian and Lozos, Julian and Beeler, Nick and Oglesby, David},
	month = apr,
	year = {2012},
	keywords = {Crystal Aging, thesis, normal stress},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/ANGXG743/Kilgore et al. - 2012 - Laboratory Observations of Fault Strength in Respo.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/7Z4AH55P/Laboratory-Observations-of-Fault-Strength-in.html:text/html},
}

@article{kilgore2017,
	title = {Rock friction under variable normal stress},
	volume = {122},
	issn = {2169-9356},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2017JB014049},
	doi = {10.1002/2017JB014049},
	abstract = {This study is to determine the detailed response of shear strength and other fault properties to changes in normal stress at room temperature using dry initially bare rock surfaces of granite at normal stresses between 5 and 7 MPa. Rapid normal stress changes result in gradual, approximately exponential changes in shear resistance with fault slip. The characteristic length of the exponential change is similar for both increases and decreases in normal stress. In contrast, changes in fault normal displacement and the amplitude of small high-frequency elastic waves transmitted across the surface follow a two stage response consisting of a large immediate and a smaller gradual response with slip. The characteristic slip distance of the small gradual response is significantly smaller than that of shear resistance. The stability of sliding in response to large step decreases in normal stress is well predicted using the shear resistance slip length observed in step increases. Analysis of the shear resistance and slip-time histories suggest nearly immediate changes in strength occur in response to rapid changes in normal stress; these are manifested as an immediate change in slip speed. These changes in slip speed can be qualitatively accounted for using a rate-independent strength model. Collectively, the observations and model show that acceleration or deceleration in response to normal stress change depends on the size of the change, the frictional characteristics of the fault surface, and the elastic properties of the loading system.},
	language = {en},
	number = {9},
	urldate = {2023-03-14},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Kilgore, Brian and Beeler, N. M. and Lozos, Julian and Oglesby, David},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/2017JB014049},
	keywords = {Crystal Aging, friction, thesis, normal stress},
	pages = {7042--7075},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/NWADZYHM/Kilgore et al. - 2017 - Rock friction under variable normal stress.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/WUTEK2DQ/2017JB014049.html:text/html},
}

@article{nordhagen2023a,
	title = {Diffusion-{Driven} {Frictional} {Aging} in {Silicon} {Carbide}},
	volume = {71},
	issn = {1573-2711},
	url = {https://doi.org/10.1007/s11249-023-01762-z},
	doi = {10.1007/s11249-023-01762-z},
	abstract = {Friction is the force resisting relative motion of objects. The force depends on material properties, loading conditions and external factors such as temperature and humidity, but also contact aging has been identified as a primary factor. Several aging mechanisms have been proposed, including increased “contact quantity” due to plastic or elastic creep and enhanced “contact quality” due to formation of strong interfacial bonds. However, comparatively less attention has been given to other mechanisms that enhance the “contact quantity”. In this study, we explore the influence of crystal faceting on the augmentation of “contact quantity” in cubic silicon carbide, driven by the minimization of surface free energy. Our observations reveal that the temporal evolution of the frictional aging effect follows a logarithmic pattern, akin to several other aging mechanisms. However, this particular mechanism is driven by internal capillary forces instead of the normal force typically associated with friction. Due to this fundamental distinction, existing frictional aging models fail to comprehensively explain the observed behavior. In light of these findings, we derive a model for the evolution of contact area caused by diffusion-driven frictional aging, drawing upon principles from statistical mechanics. Upon application of a normal force, the friction force is increased due to plastic creep. This investigation presents an alternative explanation for the logarithmic aging behavior observed and offers the potential to contribute to the development of more accurate friction models.},
	language = {en},
	number = {3},
	urldate = {2023-07-25},
	journal = {Tribology Letters},
	author = {Nordhagen, Even Marius and Sveinsson, Henrik Andersen and Malthe-Sørenssen, Anders},
	month = jul,
	year = {2023},
	keywords = {Molecular dynamics, Frictional aging, crystal rsf, Nanotribology, thesis, Faceting, Surface diffusion, prl\_paper},
	pages = {95},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/94IQG2TT/Nordhagen et al. - 2023 - Diffusion-Driven Frictional Aging in Silicon Carbi.pdf:application/pdf},
}

@misc{ho2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Friction GAN, Computer Science - Machine Learning, Statistics - Machine Learning, thesis, ddpm},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/NBEBJ8E7/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/ZM6FEYES/2006.html:text/html},
}

@misc{he2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, thesis},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/T8TL94FC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/S6ADEFEU/1512.html:text/html},
}

@article{bastek2023,
	title = {Inverse design of nonlinear mechanical metamaterials via video denoising diffusion models},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00762-x},
	doi = {10.1038/s42256-023-00762-x},
	abstract = {The accelerated inverse design of complex material properties—such as identifying a material with a given stress–strain response over a nonlinear deformation path—holds great potential for addressing challenges from soft robotics to biomedical implants and impact mitigation. Although machine learning models have provided such inverse mappings, they are typically restricted to linear target properties such as stiffness. Here, to tailor the nonlinear response, we show that video diffusion generative models trained on full-field data of periodic stochastic cellular structures can successfully predict and tune their nonlinear deformation and stress response under compression in the large-strain regime, including buckling and contact. Key to success is to break from the common strategy of directly learning a map from property to design and to extend the framework to intrinsically estimate the expected deformation path and the full-field internal stress distribution, which closely agree with finite element simulations. This work thus has the potential to simplify and accelerate the identification of materials with complex target performance.},
	language = {en},
	number = {12},
	urldate = {2024-01-15},
	journal = {Nature Machine Intelligence},
	author = {Bastek, Jan-Hendrik and Kochmann, Dennis M.},
	month = dec,
	year = {2023},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Friction GAN, Materials science, thesis, Computational science, Mechanical engineering},
	pages = {1466--1475},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/R3JK6QLK/Bastek and Kochmann - 2023 - Inverse design of nonlinear mechanical metamateria.pdf:application/pdf},
}

@article{lam2023,
	title = {Learning skillful medium-range global weather forecasting},
	volume = {382},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adi2336},
	doi = {10.1126/science.adi2336},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning–based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25° resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.
          , 
            Editor’s summary
            
              The numerical models used to predict weather are large, complex, and computationally demanding and do not learn from past weather patterns. Lam
              et al
              . introduced a machine learning–based method that has been trained directly from reanalysis data of past atmospheric conditions. In this way, the authors were able to quickly predict hundreds of weather variables globally up to 10 days in advance and at high resolution. Their predictions were more accurate than those of traditional weather models in 90\% of tested cases and displayed better severe event prediction for tropical cyclones, atmospheric rivers, and extreme temperatures. —H. Jesse Smith
            
          , 
            Machine learning leads to better, faster, and cheaper weather forecasting.},
	language = {en},
	number = {6677},
	urldate = {2024-04-24},
	journal = {Science},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = dec,
	year = {2023},
	keywords = {AIFS, weather},
	pages = {1416--1421},
	file = {Lam et al. - 2023 - Learning skillful medium-range global weather fore.pdf:/Users/evenmn/Zotero/storage/2P3EFMEX/Lam et al. - 2023 - Learning skillful medium-range global weather fore.pdf:application/pdf},
}

@misc{lang2024,
	title = {{AIFS} - {ECMWF}'s data-driven forecasting system},
	url = {http://arxiv.org/abs/2406.01465},
	doi = {10.48550/arXiv.2406.01465},
	abstract = {Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting. Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses. It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data. AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data. We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks. AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Lang, Simon and Alexe, Mihai and Chantry, Matthew and Dramsch, Jesper and Pinault, Florian and Raoult, Baudouin and Clare, Mariana C. A. and Lessig, Christian and Maier-Gerber, Michael and Magnusson, Linus and Bouallègue, Zied Ben and Nemesio, Ana Prieto and Dueben, Peter D. and Brown, Andrew and Pappenberger, Florian and Rabier, Florence},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01465 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics, AIFS},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/D5SABF8K/Lang et al. - 2024 - AIFS - ECMWF's data-driven forecasting system.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/HFSTS84Y/2406.html:text/html},
}

@article{bi2023,
	title = {Accurate medium-range global weather forecasting with {3D} neural networks},
	volume = {619},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06185-3},
	doi = {10.1038/s41586-023-06185-3},
	abstract = {Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
	language = {en},
	number = {7970},
	urldate = {2024-05-14},
	journal = {Nature},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {AIFS, Atmospheric dynamics, Computer science},
	pages = {533--538},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/Q6ZH4I69/Bi et al. - 2023 - Accurate medium-range global weather forecasting w.pdf:application/pdf},
}

@article{lillicrap2020,
	title = {Backpropagation and the brain},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2025-04-04},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cortex, Learning algorithms, Long-term potentiation, Network models, Neurophysiology},
	pages = {335--346},
}

@misc{rombach2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/TGWKR5GJ/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/9TUWHGU9/2112.html:text/html},
}

@misc{kingma2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/JVULNHDZ/Kingma og Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/WGUH8ANZ/1312.html:text/html},
}

@inproceedings{kurth2023,
	address = {New York, NY, USA},
	series = {{PASC} '23},
	title = {{FourCastNet}: {Accelerating} {Global} {High}-{Resolution} {Weather} {Forecasting} {Using} {Adaptive} {Fourier} {Neural} {Operators}},
	isbn = {979-8-4007-0190-0},
	shorttitle = {{FourCastNet}},
	url = {https://dl.acm.org/doi/10.1145/3592979.3593412},
	doi = {10.1145/3592979.3593412},
	abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
	urldate = {2025-05-16},
	booktitle = {Proceedings of the {Platform} for {Advanced} {Scientific} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
	month = jun,
	year = {2023},
	pages = {1--11},
	file = {Full Text PDF:/Users/evenmn/Zotero/storage/MSHEHGGB/Kurth et al. - 2023 - FourCastNet Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural O.pdf:application/pdf},
}

@misc{bodnar2024,
	title = {Aurora: {A} {Foundation} {Model} of the {Atmosphere}},
	shorttitle = {Aurora},
	url = {http://arxiv.org/abs/2405.13063},
	doi = {10.48550/arXiv.2405.13063},
	abstract = {Deep learning foundation models are revolutionizing many facets of science by leveraging vast amounts of data to learn general-purpose representations that can be adapted to tackle diverse downstream tasks. Foundation models hold the promise to also transform our ability to model our planet and its subsystems by exploiting the vast expanse of Earth system data. Here we introduce Aurora, a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data. Aurora leverages the strengths of the foundation modelling approach to produce operational forecasts for a wide variety of atmospheric prediction problems, including those with limited training data, heterogeneous variables, and extreme events. In under a minute, Aurora produces 5-day global air pollution predictions and 10-day high-resolution weather forecasts that outperform state-of-the-art classical simulation tools and the best specialized deep learning models. Taken together, these results indicate that foundation models can transform environmental forecasting.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Bodnar, Cristian and Bruinsma, Wessel P. and Lucic, Ana and Stanley, Megan and Brandstetter, Johannes and Garvan, Patrick and Riechert, Maik and Weyn, Jonathan and Dong, Haiyu and Vaughan, Anna and Gupta, Jayesh K. and Tambiratnam, Kit and Archibald, Alex and Heider, Elizabeth and Welling, Max and Turner, Richard E. and Perdikaris, Paris},
	month = may,
	year = {2024},
	note = {arXiv:2405.13063 [physics]},
	keywords = {mlwp, era5, global, foundation model, swin transformer, coupled},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/4GLJEMVR/Bodnar et al. - 2024 - Aurora A Foundation Model of the Atmosphere.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/A6JL77BN/2405.html:text/html},
}

@misc{wang2024a,
	title = {{ORBIT}: {Oak} {Ridge} {Base} {Foundation} {Model} for {Earth} {System} {Predictability}},
	shorttitle = {{ORBIT}},
	url = {http://arxiv.org/abs/2404.14712},
	doi = {10.48550/arXiv.2404.14712},
	abstract = {Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling efficiency maintained at 41\% to 85\% across 49,152 AMD GPUs. These breakthroughs establish new advances in AI-driven climate modeling and demonstrate promise to significantly improve the Earth system predictability.},
	urldate = {2025-04-03},
	publisher = {arXiv},
	author = {Wang, Xiao and Liu, Siyan and Tsaris, Aristeidis and Choi, Jong-Youl and Aji, Ashwin and Fan, Ming and Zhang, Wei and Yin, Junqi and Ashfaq, Moetasim and Lu, Dan and Balaprakash, Prasanna},
	month = aug,
	year = {2024},
	note = {arXiv:2404.14712 [physics]},
	keywords = {mlwp, foundation model, hpc, vision tranformer},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/4PGAI72R/Wang et al. - 2024 - ORBIT Oak Ridge Base Foundation Model for Earth S.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/D55YNLJL/2404.html:text/html},
}

@misc{nordhagen2025a,
	title = {High-{Resolution} {Probabilistic} {Data}-{Driven} {Weather} {Modeling} with a {Stretched}-{Grid}},
	url = {http://arxiv.org/abs/2511.23043},
	doi = {10.48550/arXiv.2511.23043},
	abstract = {We present a probabilistic data-driven weather model capable of providing an ensemble of high spatial resolution realizations of 87 variables at arbitrary forecast length and ensemble size. The model uses a stretched grid, dedicating 2.5 km resolution to a region of interest, and 31 km resolution elsewhere. Based on a stochastic encoder-decoder architecture, the model is trained using a loss function based on the Continuous Ranked Probability Score (CRPS) evaluated point-wise in real and spectral space. The spectral loss components is shown to be necessary to create fields that are spatially coherent. The model is compared to high-resolution operational numerical weather prediction forecasts from the MetCoOp Ensemble Prediction System (MEPS), showing competitive forecasts when evaluated against observations from surface weather stations. The model produced fields that are more spatially coherent than mean squared error based models and CRPS based models without the spectral component in the loss.},
	urldate = {2025-12-29},
	publisher = {arXiv},
	author = {Nordhagen, Even Marius and Haugen, Håvard Homleid and Salihi, Aram Farhad Shafiq and Ingstad, Magnus Sikora and Nipen, Thomas Nils and Seierstad, Ivar Ambjørn and Frogner, Inger-Lise and Clare, Mariana and Lang, Simon and Chantry, Matthew and Dueben, Peter and Kristiansen, Jørn},
	month = nov,
	year = {2025},
	note = {arXiv:2511.23043 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Physics - Atmospheric and Oceanic Physics},
	annote = {Comment: 14 pages, 8 figures},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/3IFKN9C6/Nordhagen et al. - 2025 - High-Resolution Probabilistic Data-Driven Weather Modeling with a Stretched-Grid.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/GLZNA9Q5/2511.html:text/html},
}

@article{lorenz1963,
	title = {Deterministic {Nonperiodic} {Flow}.},
	volume = {20},
	issn = {0022-4928},
	url = {https://ui.adsabs.harvard.edu/abs/1963JAtS...20..130L},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	abstract = {Finite systems of deterministic ordinary nonlinear differential equations may be designed to represent forced dissipative hydrodynamic flow. Solutions of these equations can be identified with trajectories in phase space. For those systems with bounded solutions, it is found that nonperiodic solutions are ordinarily unstable with respect to small modifications, so that slightly differing initial states can evolve into considerably different states. Systems with bounded solutions are shown to possess bounded numerical solutions.A simple system representing cellular convection is solved numerically. All of the solutions are found to be unstable, and almost all of them are nonperiodic.The feasibility of very-long-range weather prediction is examined in the light of these results.},
	urldate = {2025-01-27},
	journal = {Journal of the Atmospheric Sciences},
	author = {Lorenz, Edward N.},
	month = mar,
	year = {1963},
	note = {Publisher: AMS
ADS Bibcode: 1963JAtS...20..130L},
	keywords = {butterfly effect},
	pages = {130--148},
	annote = {The Butterfly Effect
},
	file = {lorenz1962.pdf:/Users/evenmn/Zotero/storage/EJRDAZEC/lorenz1962.pdf:application/pdf},
}

@misc{lang2024a,
	title = {{AIFS}-{CRPS}: {Ensemble} forecasting using a model trained with a loss function based on the {Continuous} {Ranked} {Probability} {Score}},
	shorttitle = {{AIFS}-{CRPS}},
	url = {http://arxiv.org/abs/2412.15832},
	doi = {10.48550/arXiv.2412.15832},
	abstract = {Over the last three decades, ensemble forecasts have become an integral part of forecasting the weather. They provide users with more complete information than single forecasts as they permit to estimate the probability of weather events by representing the sources of uncertainties and accounting for the day-to-day variability of error growth in the atmosphere. This paper presents a novel approach to obtain a weather forecast model for ensemble forecasting with machine-learning. AIFS-CRPS is a variant of the Artificial Intelligence Forecasting System (AIFS) developed at ECMWF. Its loss function is based on a proper score, the Continuous Ranked Probability Score (CRPS). For the loss, the almost fair CRPS is introduced because it approximately removes the bias in the score due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The trained model is stochastic and can generate as many exchangeable members as desired and computationally feasible in inference. For medium-range forecasts AIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS) ensemble for the majority of variables and lead times. For subseasonal forecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is competitive with the IFS ensemble when forecasts are evaluated as anomalies to remove the influence of model biases.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Lang, Simon and Alexe, Mihai and Clare, Mariana C. A. and Roberts, Christopher and Adewoyin, Rilwan and Bouallègue, Zied Ben and Chantry, Matthew and Dramsch, Jesper and Dueben, Peter D. and Hahner, Sara and Maciel, Pedro and Prieto-Nemesio, Ana and O'Brien, Cathal and Pinault, Florian and Polster, Jan and Raoult, Baudouin and Tietsche, Steffen and Leutbecher, Martin},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15832 [physics]},
	keywords = {ensembles, mlwp, era5, global, gnn, crps},
	annote = {ECMWF
},
	annote = {Main take aways:


Train with a modified loss (almost fair CRPS)


Train and initialize ensemble members from deterministic ERA5


Gaussian processor grids


Impossible to see if they have the same problem as us with different precipitation fields


Training on H100 Malenustraum5 in Spain


},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/QJRL8ALG/Lang et al. - 2024 - AIFS-CRPS Ensemble forecasting using a model trai.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/4BD5FA8R/2412.html:text/html},
}

@misc{bonev2025,
	title = {{FourCastNet} 3: {A} geometric approach to probabilistic machine-learning weather forecasting at scale},
	shorttitle = {{FourCastNet} 3},
	url = {http://arxiv.org/abs/2507.12144},
	doi = {10.48550/arXiv.2507.12144},
	abstract = {FourCastNet 3 advances global weather modeling by implementing a scalable, geometric machine learning (ML) approach to probabilistic ensemble forecasting. The approach is designed to respect spherical geometry and to accurately model the spatially correlated probabilistic nature of the problem, resulting in stable spectra and realistic dynamics across multiple scales. FourCastNet 3 delivers forecasting accuracy that surpasses leading conventional ensemble models and rivals the best diffusion-based methods, while producing forecasts 8 to 60 times faster than these approaches. In contrast to other ML approaches, FourCastNet 3 demonstrates excellent probabilistic calibration and retains realistic spectra, even at extended lead times of up to 60 days. All of these advances are realized using a purely convolutional neural network architecture tailored for spherical geometry. Scalable and efficient large-scale training on 1024 GPUs and more is enabled by a novel training paradigm for combined model- and data-parallelism, inspired by domain decomposition methods in classical numerical models. Additionally, FourCastNet 3 enables rapid inference on a single GPU, producing a 60-day global forecast at 0.25\{{\textbackslash}deg\}, 6-hourly resolution in under 4 minutes. Its computational efficiency, medium-range probabilistic skill, spectral fidelity, and rollout stability at subseasonal timescales make it a strong candidate for improving meteorological forecasting and early warning systems through large ensemble predictions.},
	urldate = {2025-08-13},
	publisher = {arXiv},
	author = {Bonev, Boris and Kurth, Thorsten and Mahesh, Ankur and Bisson, Mauro and Kossaifi, Jean and Kashinath, Karthik and Anandkumar, Anima and Collins, William D. and Pritchard, Michael S. and Keller, Alexander},
	month = jul,
	year = {2025},
	note = {arXiv:2507.12144 [cs]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/IZZQ6QYE/Bonev et al. - 2025 - FourCastNet 3 A geometric approach to probabilistic machine-learning weather forecasting at scale.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/LLUE63BF/2507.html:text/html},
}

@article{gneiting2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
	number = {477},
	urldate = {2025-08-13},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/016214506000001437},
	keywords = {Bayes factor, Bregman divergence, Brier score, Coherent, Continuous ranked probability score, Cross-validation, Entropy, Kernel score, Loss function, Minimum contrast estimation, Negative definite function, Prediction interval, Predictive distribution, Quantile forecast, Scoring rule, Skill score, Strictly proper, Utility function},
	pages = {359--378},
}
