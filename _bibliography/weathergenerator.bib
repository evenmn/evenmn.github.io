
@misc{bodnar2024,
	title = {Aurora: {A} {Foundation} {Model} of the {Atmosphere}},
	shorttitle = {Aurora},
	url = {http://arxiv.org/abs/2405.13063},
	doi = {10.48550/arXiv.2405.13063},
	abstract = {Deep learning foundation models are revolutionizing many facets of science by leveraging vast amounts of data to learn general-purpose representations that can be adapted to tackle diverse downstream tasks. Foundation models hold the promise to also transform our ability to model our planet and its subsystems by exploiting the vast expanse of Earth system data. Here we introduce Aurora, a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data. Aurora leverages the strengths of the foundation modelling approach to produce operational forecasts for a wide variety of atmospheric prediction problems, including those with limited training data, heterogeneous variables, and extreme events. In under a minute, Aurora produces 5-day global air pollution predictions and 10-day high-resolution weather forecasts that outperform state-of-the-art classical simulation tools and the best specialized deep learning models. Taken together, these results indicate that foundation models can transform environmental forecasting.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Bodnar, Cristian and Bruinsma, Wessel P. and Lucic, Ana and Stanley, Megan and Brandstetter, Johannes and Garvan, Patrick and Riechert, Maik and Weyn, Jonathan and Dong, Haiyu and Vaughan, Anna and Gupta, Jayesh K. and Tambiratnam, Kit and Archibald, Alex and Heider, Elizabeth and Welling, Max and Turner, Richard E. and Perdikaris, Paris},
	month = may,
	year = {2024},
	note = {arXiv:2405.13063 [physics]},
	keywords = {coupled, era5, foundation model, global, mlwp, swin transformer},
	file = {arXiv Fulltext PDF:/Users/evenmn/Zotero/storage/4GLJEMVR/Bodnar et al. - 2024 - Aurora A Foundation Model of the Atmosphere.pdf:application/pdf;arXiv.org Snapshot:/Users/evenmn/Zotero/storage/A6JL77BN/2405.html:text/html},
}

@misc{wang2024a,
	title = {{ORBIT}: {Oak} {Ridge} {Base} {Foundation} {Model} for {Earth} {System} {Predictability}},
	shorttitle = {{ORBIT}},
	url = {http://arxiv.org/abs/2404.14712},
	doi = {10.48550/arXiv.2404.14712},
	abstract = {Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling efficiency maintained at 41\% to 85\% across 49,152 AMD GPUs. These breakthroughs establish new advances in AI-driven climate modeling and demonstrate promise to significantly improve the Earth system predictability.},
	urldate = {2025-04-03},
	publisher = {arXiv},
	author = {Wang, Xiao and Liu, Siyan and Tsaris, Aristeidis and Choi, Jong-Youl and Aji, Ashwin and Fan, Ming and Zhang, Wei and Yin, Junqi and Ashfaq, Moetasim and Lu, Dan and Balaprakash, Prasanna},
	month = aug,
	year = {2024},
	note = {arXiv:2404.14712 [physics]},
	keywords = {foundation model, hpc, mlwp, vision tranformer},
	file = {Preprint PDF:/Users/evenmn/Zotero/storage/4PGAI72R/Wang et al. - 2024 - ORBIT Oak Ridge Base Foundation Model for Earth S.pdf:application/pdf;Snapshot:/Users/evenmn/Zotero/storage/D55YNLJL/2404.html:text/html},
}
