---
layout: post
title: Bris
subtitle: High-Resolution Probabilistic Data-Driven Weather Modeling
cover-img: /assets/img/bris/diana.png
thumbnail-img: /assets/img/bris/bris.png
share-img: /assets/img/bris/bris.png
gh-repo: evenmn/evenmn.github.io
gh-badge: [follow]
tags: [Bris, weather, forecasting, machine learning, data-driven models]
comments: true
mathjax: true
author: Even Nordhagen
---

Can we replicate recent advances in data-driven weather forecasting at high spatial resolution?

In recent years, data-driven weather models have demonstrated impressive predictive skill at a fraction of the computational cost of traditional numerical weather prediction (NWP) systems. However, most of these models are trained on ERA5 reanalysis data at a relatively coarse horizontal resolution of about 31 km. This raises an important question: can we achieve similar performance in a high-resolution setting that captures finer-scale weather phenomena?

We present Bris, a high-resolution probabilistic data-driven weather model trained on the MetCoOp Ensemble Prediction System (MEPS) analysis at 2.5 km resolution. The name "Bris" means *breeze* in Norwegian and is pronounced similarly—no abbreviation or acronym. This work is a collaboration between the Norwegian Meteorological Institute and the European Centre for Medium-range Weather Forecasting (ECMWF).

To produce physically realistic weather scenarios, we developed a training objective that combines physical and spectral probabilistic criteria. The resulting model matches the forecast skill of MEPS while running 20 to 30 times faster. In this post, we explore the key ideas behind Bris’ design, training, and performance, as well as the path forward.

I aim to explain the concepts in clear, accessible language. For a more technical treatment, see [our recent preprint {% cite nordhagen2025a %}](http://arxiv.org/abs/2511.23043v). The methodology builds on the stretched-grid approach discussed in [an earlier post](https://evennordhagen.com/2024-09-12-stretched-grid/).

<div class="text-center">
    <img src="/assets/img/bris/diana.png" alt="Bris" style="width: 70%;">
</div>
*Example field*

## The case for high resolution

Why invest effort in high-resolution models? Aren’t current state-of-the-art models at coarser resolutions sufficient?

The answer depends on the weather variable and the application. Some phenomena, like rain showers and local wind gusts, vary significantly over small distances. Capturing this variability requires high spatial resolution. This is why many national meteorological and hydrological services (NMHSes) develop and operate high-resolution models.

Conversely, other variables, such as mean sea level pressure, primarily reflect large-scale atmospheric patterns. For these, increasing resolution yields little improvement. To illustrate this, we compare Bris (2.5 km resolution) with ECMWF’s data-driven model AIFS-CRPS at 31 km {% cite lang2024a %}:

<div class="text-center">
    <img src="/assets/img/bris/crps.png" alt="Bris" style="width: 70%;">
</div>
*Continuous Ranked Probability Score (CRPS) for 2m temperature, 10m wind speed, 6-hour accumulated precipitation, and mean sea level pressure. Bris (high resolution) is compared to AIFS-CRPS (lower resolution) against synoptic stations in Norway. Lower scores indicate better performance.*

CRPS is a proper scoring rule for probabilistic forecasts; think of it as a measure of prediction accuracy that accounts for uncertainty. Bris shows clear improvements in temperature, wind speed, and precipitation forecasts but performs slightly worse on mean sea level pressure. Since AIFS-CRPS is a state-of-the-art global data-driven model, this comparison highlights the value of high resolution for certain variables.

High resolution is especially important for predicting extreme weather events like heavy precipitation and storms, which threaten millions in Europe annually. Given that saving lives and property is a core NMHS responsibility, high-resolution probabilistic models are essential.

<video autoplay loop muted playsinline preload="metadata" width="720" controls playsinline muted>
  <source src="/assets/img/bris/lorenz_web.mp4" type="video/mp4">
</video>
*The Lorenz system illustrates atmospheric chaos and sensitivity to initial conditions. Here, two nearly identical initial states (blue trajectory perturbed by 0.00000001 in x-direction relative to orange) diverge rapidly. This sensitivity is known as the butterfly effect.*

## Why probabilistic forecasting?

Weather prediction models can be deterministic (producing a single forecast) or probabilistic, which estimate the likelihood of multiple possible outcomes.

The atmosphere is a chaotic system influenced by inherently unpredictable stochastic processes such as turbulence. In chaos theory terms, the atmosphere is a level 1 chaotic system, meaning its future state is governed by random processes beyond our control. For comparison, the stock market is often considered a level 2 chaotic system, where our current actions influence future dynamics.

Even if the atmosphere were entirely deterministic, measurement errors and incomplete knowledge of initial conditions create uncertainty. Edward Lorenz famously demonstrated this with a set of nonlinear differential equations showing that tiny changes in initial conditions can lead to vastly different outcomes — the butterfly effect (see animation above). This sensitivity limits the predictability horizon of weather forecasts.

Because of these factors, forecasting probabilities rather than single outcomes better reflects the true uncertainty. Probabilistic forecasts provide three key benefits:

1. Quantification of uncertainty
2. Sharp, physically consistent fields
3. Ability to capture rare extreme events

<div class="text-center">
    <img src="/assets/img/bris/precip_fields.png" alt="Bris" style="width: 70%;">
</div>
*Precipitation fields over parts of the Nordics at 2.5 km resolution. Panels show: (a) MEPS (reference), (b) deterministic model, (c) Bris trained with a simplified loss, and (d) Bris with full probabilistic training. Note the smoothing in (b) and noise in (c).*

Probabilistic forecasts are often implemented as ensemble prediction systems (EPS), which generate multiple realizations to represent possible future states. Think of ensemble members as trajectories like the blue and orange lines in the Lorenz system animation. Each member is considered equally likely, and probabilities are derived by clustering the ensemble.

%Deterministic models typically minimize mean squared error, producing forecasts that approximate the average of all possible outcomes. However, this often leads to unrealistic, overly smooth forecasts that do not correspond to any physically plausible scenario—a problem known as the double penalty effect.

## How Bris works
Bris is based on an encoder-processor-decoder design, where each of the components are graph neural networks. 

<div class="text-center">
    <img src="/assets/img/bris/model.pdf" alt="Bris" style="width: 70%;">
</div>
*Schematic of the encoder-processor-decoder design and ensemble training. (a) Inputs are past ($$t_{−6h}$$) and current ($$t_{0h}$$) states from MEPS and ERA5, replicated M times for ensemble members. (b) The encoder-processor-decoder architecture injects noise (z) into the latent space to model stochasticity. (c) Predictions are evaluated using point-wise and spectral loss functions.*

### Model architecture

Bris follows an encoder-processor-decoder structure common in recent data-driven weather models {% cite bi2023 bonev2025 lam2023 lang2024a %}.

- **Encoder:** Transforms input atmospheric fields into a latent representation.
- **Processor:** Exchanges information across variables and spatial locations within the latent space.
- **Decoder:** Maps the latent state back to physical space to produce forecasts.

Bris takes the states as -6h and 0h prior to the issued time as inputs. These input states have dimensionality $$(2, N_{grid}, N_{var})$$, where $$N_{grid}=1,400,000$$ is the number of grid points and $$N_{var}=103$$ is the number of variables. The encoder transforms these inputs to a latent representation of shape $$(N_{mesh}, N_{channels})$$, where $$N_{mesh}=284,000$$ is the number of latent points and $$N_{channels}=1,024$$ is the number of channels. Note how the dimensions are changed: the number of spatial points decrease, but the depth of each spatial point increases. This design has two purposes:

1. The variables get mixed, and the model can learn different kinds of correlation
2. A smaller position dimension is beneficial when information is propagated in space (which is the case for the latent space)

Also, notice that there's hardly any compression of the inputs: the total size of the latent space is similar to the total size of the inputs. When comparing to autoencoders, this might be surprising, but our experience is that the model largely benefits from a rich latent space. Possible explanation: Transformer blocks work best with linear dynamics. Since atmospheric dynamics are largely non-linear, the model can approximate the dynamics as linear. This requires more space, but analysis are done more efficiently on the linear approximations.

<div class="text-center">
    <img src="/assets/img/bris/gnn.png" alt="GNN" style="width: 50%;">
</div>
*The model is a graph neural network, where grid nodes (b) are connected to latent nodes (d) using edges (c), which forms the encoder. Information propagates in space, thanks to processor edges connecting the latent nodes. Predictions are generated by mapping the latent state back to the grid with decoder edges (e).*

### Graph neural network (GNN)

The model operates on a graph where nodes represent spatial locations and edges connect points. Each point/pixel in the input fields are treated as a node, and similarly the spatial points in the latent space are nodes. The encoder is simply 12 edges connecting a latent node connected to the 12 nearest grid nodes, while the processor is edges connecting the latent nodes. The decoder consists of 3 edges, connecting each grid node to the 3 nearest mesh nodes (see figure above). 

Crucially, all the model’s trainable parameter reside inside the edges, where all the encoder edges share one set of parameter, all the processor edges share one set of parameters and the decoder edges share one set of parameters. With this design, the model aim to become position agnostic, where the exact same encoder acts on inputs in Argentina as in Norway. This is important for two reasons:

1. Allows the model to generalize to various regions, terrains and climates.
2. The model lives in the edges, and does not depend on the graph.

This last point is crucial, as it lets us change graphs under training and inference. We utilize this by first pretraining the model on a global domain, and then fine-tune on a high resolution regional domain. The flexibility of the model can also be used to run the model with high resolution elsewhere.


### Ensemble method and training objective

To generate an ensemble of predictions, we can technically just run the our encoder-processor-decoder model in parallel. However, this is not in particular useful if the generated members are not different. To ensure unique members, we introduce a noise injector that perturbes the latent space. Physically, the perturbations represent stochastic processes in the atmosphere, like discussed above. 

A well calibrated ensemble has small spread when the predictability is high, and, conversely, large spread for uncertain events. More precisely, assuming that the ensemble follows a normal distribution, the ensemble spread should at all times equal the root mean-squared error between the ensemble mean and all members. During training, the model is optimized to fullfill this desire, as we will discuss shortly. The full ensemble training procedure is presented in the following figure:

<div class="text-center">
    <img src="/assets/img/bris/model.pdf" alt="Bris" style="width: 850%;">
</div>
*Schematic of the encoder-processor-decoder design and ensemble training. (a) Inputs are past ($$t_{−6h}$$) and current ($$t_{0h}$$) states from MEPS and ERA5, replicated M times for ensemble members. (b) The encoder-processor-decoder architecture injects noise (z) into the latent space to model stochasticity. (c) Predictions are evaluated using point-wise and spectral loss functions.*


#### Training objective

The Continuous Ranked Probability Score (CRPS) is a proper scoring rule, meaning that the metric is minimized if and only if the predicted distribution equals the target distribution. For a predictive cumulative distribution function (CDF) $$F$$ and an observed value $$y$$, the CRPS is defined as

$$
\mathrm{CRPS}(F, y)
=
\int_{-\infty}^{\infty}
\left(
F(z) - \mathbf{1}\{z \ge y\}
\right)^2 \, \mathrm{d}z,
$$

where $$\mathbf{1}\{\cdot\}$$ denotes the indicator function.

For an ensemble, which represents the probability distribution by discrete numbers, $$\{x_1, \dots, x_M\},$$ in which case the CRPS can be decomposed into an accuracy and spread term {% cite gneiting2007 %}:

$$
\mathrm{CRPS}(\{x\}, y)
=
\frac{1}{M} \sum_{m=1}^{M} |x_m - y|
\;-\;
\frac{1}{2M^2}
\sum_{m=1}^{M}
\sum_{n=1}^{M}
|x_m - x_n|.
$$

By using this as the training objective, we will simultaneously optimize spread and skill *point-wise in the fields*. Point-wise loss evaluation is typically done with mean-squared error for deterministic models, with success. When doing the same with CRPS across ensemble members, we only encourage correct marginal distributions of the points, and we cannot guarantee physical realizations. In facts, the fields tend to get noisy when only point-wise CRPS is applied, like shown in the precipitation fields above.

To avoid this issue, we need to incorporate spatial information into the training objective. There are many ways of doing this, where our approach is about evaluating the fields frequency-wise in addition to the conventional point-wise evaluation. For this spectral loss we use fast Fourier transform (FFT), which transforms the predictions and targets to the frequency domain without loss of information. Since all information is conserved, so must the information about spread, and we can evaluate CRPS directly on the frequency domain. Our total loss is split in a global and regional part and reads:

$$
\mathcal{L}(\{x\},y)
=
\sum_v\sum_tw_v\left[
    \underbrace{\mathrm{CRPS}(\{x_{\mathrm{g}}\},y_{\mathrm{g}})}_{\mathrm{global}}
    +
    \lambda_{\mathrm{r}}
    \Big[
        \underbrace{\mathrm{CRPS}(\{x_{\mathrm{r}}\},y_{\mathrm{r}})
        +
        \lambda_{\mathrm{f}}
        \mathrm{CRPS}(\{\mathrm{FFT}(x_{\mathrm{r}})\},\mathrm{FFT}(y_{\mathrm{r}}))}_{\mathrm{regional}}
    \Big]
\right].
$$

Here, ...


## Comparison with NWP

In this last section, we will compare the performance of Bris against the state-of-the-art ensemble prediction system MEPS. 

Bris achieves comparable forecast skill to MEPS, the operational high-resolution ensemble prediction system, while running significantly faster. This speed-up opens possibilities for rapid forecasting and ensemble generation on limited computational resources.

We are actively working to integrate Bris into operational workflows to complement existing NWP systems.

<div class="text-center">
    <img src="/assets/img/bris/spreadrmse.png" alt="Bris" style="width: 80%;">
</div>
*Spreadskill.*

## Conclusions

The WeatherGenerator project aims to develop next-generation weather forecasting models that combine high resolution, probabilistic outputs, and computational efficiency. This is vital for improving warnings of increasingly frequent extreme weather events and maintaining Europe’s leadership in global weather prediction.

Europe’s rich meteorological datasets and high-performance computing infrastructure provide a strong foundation for achieving these goals.

For more details, see the official ECMWF blog post.

---

**DOI:** https://doi.org/10.5281/zenodo.1234567  
**Archived version:** Zenodo  
**Canonical version:** https://evennordhagen.com/2025-12-12-bris/

![Signature](/assets/img/signature.png){: .mx-auto.d-block :}

Feel free to leave a comment or question below.

*The WeatherGenerator project (grant agreement No. 101187947) is funded by the European Union. Views and opinions expressed are those of the author(s) only and do not necessarily reflect those of the European Union or the Commission.*

![Funded by EU](/assets/img/weather-generator/EN-Funded by the EU-POS.png){: .mx-auto.d-block :}

## References
{% bibliography --file bris %}
